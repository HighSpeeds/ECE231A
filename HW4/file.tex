\include{"../preamble.tex"}
\title{ECE 231A HW 3}
\begin{document}
\maketitle
\section*{Problem 1}
\subsection*{(a)}
Then we have
$$I(X;Y|Z)=H(Y|Z)-H(Y|X,Z)=H(Y|Z)=\alpha H(p)$$
Thus we have that the capacity is $\alpha$
\subsection*{(b)}
We have 
\begin{align*}
    I(X;Y)&=H(Y)-H(Y|X)\\
    &=-p\alpha \log_2(p\alpha)-(1-p\alpha) \log_2((1-p\alpha))-H(Y|X)\\
    &=-p\alpha \log_2(p\alpha)-(1-p\alpha) \log_2((1-p\alpha))-pH(\alpha)
\end{align*}
To find its maximum, we take its derivative and set it to 0
\begin{align*}
    \frac{\partial I(X;Y)}{\partial p}&=0\\
    -\alpha \log_2(p\alpha)-\frac{\alpha}{\ln(2)}+\alpha \log_2(1-p\alpha)+\frac{\alpha}{\ln(2)}-H(\alpha)&=0\\
    -H(\alpha)&= \alpha\log_2(p\alpha)-\alpha \log_2(1-p\alpha)\\
    2^{\frac{-H(\alpha)}{\alpha}}&=\frac{p\alpha}{1-p\alpha}\\
    p&=\boxed{\frac{1}{\alpha(2^{\frac{H(\alpha)}{\alpha}}+1)}}
\end{align*}
\section*{Problem 2}
\subsection*{(a)}
We have that,
\begin{align*}
    I(X;Y|S)&=H(Y|S)-H(Y|X,S)\\
    &=H(Y,S)-H(S)-H(Y,X,S)-H(S,X)\\
    &=I(X,S;Y,S)-H(S)
\end{align*}
And thus 
$$I(X,S;Y,S)=I(X;Y|S)+H(S)$$
\subsection*{(b)}
We have that 
\begin{align*}
    C&=\max_{p_S(s)}\left(\max_{p_X(x|s)}\left(I(X,S;Y,S)\right)\right)\\
    &=\max_{p_S(s)}\left(\max_{p_X(x|s)}\left(I(X;Y|S)+H(S)\right)\right)\\
    &=\max_{p_S(s)}\left(H(S)+\max_{p_X(x|s)}\left(I(X;Y|S)\right)\right)\\
    &=\max_{p_S(s)}\left(H(S)+\max_{p_X(x|s)}\left(H(Y|S)+H(X|S)-H(Y,X|S)\right)\right)\\
    &=\max_{p_S(s)}\left(H(S)+\max_{p_X(x|s)}\left(\sum_{s=1}^{K}p_S(s)(H(Y|S=s)+H(X|S=s)-H(Y,X|S=s))\right)\right)\\
    &=\max_{p_S(s)}\left(H(S)+\sum_{s=1}^{K}p_S(s)C_s\right)
\end{align*}
\subsection*{(c)}
We have that we want the following constraint to be true
$$\sum_{s=1}^{K}p_S(s)=1$$
So then with a lagrangian we have
$$\frac{\partial}{\partial p_S(s)}\left(H(S)+\sum_{s=1}^{K}p_S(s)C_s+\lambda\sum_{s=1}^{K}p_S(s)-1\right)=-\log_2(p_S(s))-\frac{1}{\ln(2)}+C_s+\lambda$$
Setting this equal to 0 we get that 
$$p_S*(s)=2^{-\frac{1}{\log2(e)}}2^{\lambda}2^{C_s}$$
since $\sum_{s=1}^{K}p_S*(s)=1$, we have
$$2^{\lambda}=\frac{1}{2^{-\frac{1}{\log2(e)}}\sum_{s=1}^{K}2^{C_s}}$$
And thus we have 
$$p_S*(s)=\boxed{\frac{2^{C_s}}{\sum_{s=1}^{K}2^{C_s}}}$$
\subsection*{(d)}
Then we have that 
\begin{align*}
    H(S)&=-\sum_{s=1}^{K}p_S(s)\log_2(p_S(s))\\
    &=\log_2\left(\sum_{s=1}^{K}2^{C_s}\right)-\sum_{s=1}^{K}p_S(s) C_s
\end{align*}
Thus we have that
$$C=H(S)+\sum_{s=1}^{K}p_S(s) C_s=\log_2\left(\sum_{s=1}^{K}2^{C_s}\right)$$
\section*{Problem 3}
\subsection*{(a)}
We want the outputs to $Y_1$ and $Y_2$ to be totally
dependent on the input $X$. Thus we have that 
$$I(S,Y_1,Y_2)=H(S)-H(S|Y_1,Y_2)=H(S)$$
\subsection*{(b)}
Since $X_1$ is a BEC channel, the maximum
distribution is just $P(X_1=0)=P(X_1=1)=\frac{1}{2}$. 
This will give us that the capacity of this channel is 
$1-\alpha$. Thus we have that the minimum capacity
of the noiseless channel is $\alpha$.
\subsection*{(c)}
Send one symbol through the
erasure channel and one through the noiseless channel, 
then if the fraction of the symbols through the 
noiseless is greater than $\alpha$, we send the next
symbol through the noisy channel, and if it 
is less than $\alpha$, we send the next symbol
through the noiseless channel. This will
converge to $\alpha$ with time.
\subsection*{(d)}
We have that
\begin{align*}
    I(X_1,Y_1)&=H(X_1)-H(X_1|Y_1)\\
    &=(1-\alpha)H(X_1)
\end{align*}
We want to maximize this subject
to the constrainIts on $P(X_1=1)=p$
$$p\geq 0$$
$$1-p\leq 1$$
$$E[q(X_1)]=3p+2(1-p)=2+p\leq Q$$
Then with the first two conditions will be inactive constraints
since entropy gets bigger as $p$ gets closer to $\frac{1}{2}$,
but the third one will be active. Thus we have that
$$f=(1-\alpha)H(p)+\lambda (2+p-Q)$$
$$\frac{\partial f}{\partial p}=(1-\alpha)H'(p)+\lambda=0$$
$$\frac{\partial f}{\partial \lambda}=2+p-Q=0$$
Thus we have that
$$\log_2\left(\frac{p}{1-p}\right)=\frac{\lambda}{1-\alpha}$$
$$p=\frac{1}{1+2^{-\frac{\lambda}{1-\alpha}}}$$
plugging this back into the third constraint we get that
$$p=Q-2$$
$$1=(Q-2)(1+2^{-\frac{\lambda}{1-\alpha}})$$
$$\frac{3-Q}{Q-2}=2^{-\frac{\lambda}{1-\alpha}}$$
$$\lambda=(1-\alpha)\log_2\left(\frac{Q-2}{3-Q}\right)$$
Thus we get that
$$p=3-Q$$
and thus the maximum $I(X_1,Y_1)$ is $(1-\alpha)H_2(3-Q)$, 
and thus the maximum of $I(S;Y_1)$ is $\boxed{(1-\alpha)H_2(3-Q)}$
as well.
\subsection*{(e)}
Then we must have the rate of 
the other channel must be $1-(1-\alpha)H_2(3-Q)$
\section*{Problem 4}
\subsection*{(a)}
We can intuitively see that the stationary distribution
is $\frac{1}{3}$ for all, thus we have that the rate 
is 
$$\mathcal{H}(S)=H_3(q,q,1-2q)$$
Thus in order for lossless coding, we must have that q must
satisfy the following
$$R=1-\alpha>\mathcal{H}(S)$$
$$\boxed{1-\alpha>H_2(2q,1-2q)+2q}$$
\subsection*{(b)}
From the source coding theorem we have that 
$$1-\alpha>\beta\mathcal{H}(S)$$
Thus the maximum beta is
$$\beta=\boxed{\frac{2(1-\alpha)}{3}}$$
\section*{Problem 5}
\subsection*{(a)}
We have that 
$$I(X;Y)=H(X)-H(X|Y)=H(X)-pH(X|Y=E)-(1-p)H(X|Y=X)$$
$$I(X;Y)=H(X)-pH(X|Y=E)=(1-p)H(X)$$
thus to maximize we just have all the values of $X$ be equally likely
thus we will have
$$\boxed{C=(1-p)\log_2(L)}$$
\subsection*{(b)}
Since feedback cannot increase the capacity of the channel, we have that
the capacity still is $\boxed{(1-p)\log_2(L)}$
\subsection*{(c)}
Once again since feedback cannot increase the capacity of the channel, we have that
the capacity still is $\boxed{(1-p)\log_2(L)}$

\end{document}